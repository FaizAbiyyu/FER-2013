{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cebcceca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3357 - accuracy: 0.9053 - val_loss: 0.1428 - val_accuracy: 0.9585\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1073 - accuracy: 0.9674 - val_loss: 0.1115 - val_accuracy: 0.9647\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0687 - accuracy: 0.9786 - val_loss: 0.0972 - val_accuracy: 0.9710\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0461 - accuracy: 0.9856 - val_loss: 0.1013 - val_accuracy: 0.9688\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.0342 - accuracy: 0.9890 - val_loss: 0.0936 - val_accuracy: 0.9752\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0230 - accuracy: 0.9923 - val_loss: 0.0907 - val_accuracy: 0.9757\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.1199 - val_accuracy: 0.9707\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.0124 - accuracy: 0.9959 - val_loss: 0.1301 - val_accuracy: 0.9707\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0135 - accuracy: 0.9953 - val_loss: 0.1223 - val_accuracy: 0.9718\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0097 - accuracy: 0.9967 - val_loss: 0.1175 - val_accuracy: 0.9753\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1297 - accuracy: 0.9747\n",
      "Test accuracy: 0.9746999740600586\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fungsi untuk menghitung LBP pada gambar\n",
    "def compute_lbp(image):\n",
    "    radius = 1\n",
    "    n_points = 8 * radius\n",
    "    lbp_image = local_binary_pattern(image, n_points, radius, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Memuat data gambar dan label (misalnya dataset MNIST)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# lenet\n",
    "# Menghitung LBP pada setiap gambar dalam data pelatihan dan pengujian\n",
    "x_train_lbp = np.array([compute_lbp(image) for image in x_train])\n",
    "x_test_lbp = np.array([compute_lbp(image) for image in x_test])\n",
    "\n",
    "# Normalisasi nilai piksel menjadi rentang [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Menambahkan dimensi saluran (channel) untuk LBP\n",
    "x_train_lbp = x_train_lbp.reshape(x_train_lbp.shape + (1,))\n",
    "x_test_lbp = x_test_lbp.reshape(x_test_lbp.shape + (1,))\n",
    "\n",
    "# Menggabungkan hasil LBP dengan citra asli\n",
    "x_train_combined = np.concatenate((x_train[..., np.newaxis], x_train_lbp), axis=3)\n",
    "x_test_combined = np.concatenate((x_test[..., np.newaxis], x_test_lbp), axis=3)\n",
    "\n",
    "# Membagi data menjadi data pelatihan dan validasi\n",
    "x_train_combined, x_valid_combined, y_train, y_valid = train_test_split(\n",
    "    x_train_combined, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Membangun model CNN\n",
    "model = tf.keras.Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 2)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Melatih model\n",
    "model.fit(x_train_combined, y_train, epochs=10, validation_data=(x_valid_combined, y_valid))\n",
    "\n",
    "# Evaluasi model pada data pengujian\n",
    "test_loss, test_acc = model.evaluate(x_test_combined, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4d567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "718/718 [==============================] - 9s 5ms/step - loss: 2.0048 - accuracy: 0.3336 - val_loss: 1.5477 - val_accuracy: 0.4054\n",
      "Epoch 2/10\n",
      "718/718 [==============================] - 3s 4ms/step - loss: 1.3740 - accuracy: 0.4853 - val_loss: 1.5119 - val_accuracy: 0.4267\n",
      "Epoch 3/10\n",
      "718/718 [==============================] - 3s 4ms/step - loss: 1.1615 - accuracy: 0.5693 - val_loss: 1.5441 - val_accuracy: 0.4303\n",
      "Epoch 4/10\n",
      "718/718 [==============================] - 3s 4ms/step - loss: 0.9479 - accuracy: 0.6539 - val_loss: 1.7162 - val_accuracy: 0.4051\n",
      "Epoch 5/10\n",
      "718/718 [==============================] - 3s 4ms/step - loss: 0.7097 - accuracy: 0.7454 - val_loss: 1.9752 - val_accuracy: 0.3868\n",
      "Epoch 6/10\n",
      "718/718 [==============================] - 3s 4ms/step - loss: 0.5063 - accuracy: 0.8257 - val_loss: 2.2470 - val_accuracy: 0.4002\n",
      "Epoch 7/10\n",
      "718/718 [==============================] - 3s 4ms/step - loss: 0.3326 - accuracy: 0.8899 - val_loss: 2.7315 - val_accuracy: 0.3948\n",
      "Epoch 8/10\n",
      "718/718 [==============================] - 3s 4ms/step - loss: 0.2238 - accuracy: 0.9296 - val_loss: 3.0522 - val_accuracy: 0.3948\n",
      "Epoch 9/10\n",
      "718/718 [==============================] - 3s 4ms/step - loss: 0.1654 - accuracy: 0.9489 - val_loss: 3.5798 - val_accuracy: 0.4000\n",
      "Epoch 10/10\n",
      "718/718 [==============================] - 3s 4ms/step - loss: 0.1248 - accuracy: 0.9635 - val_loss: 3.9147 - val_accuracy: 0.3884\n",
      "225/225 [==============================] - 1s 2ms/step - loss: 4.0494 - accuracy: 0.3817\n",
      "Test accuracy: 0.3817219138145447\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fungsi untuk menghitung LBP pada gambar\n",
    "def compute_lbp(image):\n",
    "    radius = 1\n",
    "    n_points = 8 * radius\n",
    "    lbp_image = local_binary_pattern(image, n_points, radius, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Memuat dataset FER2013 dari CSV\n",
    "dataset = pd.read_csv('../fer2013.csv')\n",
    "\n",
    "# Konversi kolom 'pixels' ke citra dan kolom 'emotion' menjadi label\n",
    "pixels = dataset['pixels'].apply(lambda x: np.array(x.split(), dtype='uint8').reshape(48, 48))\n",
    "labels = dataset['emotion']\n",
    "\n",
    "# Menghitung LBP pada setiap gambar dalam dataset\n",
    "lbp_images = np.array([compute_lbp(image) for image in pixels])\n",
    "\n",
    "# Normalisasi nilai piksel menjadi rentang [0, 1]\n",
    "pixels = pixels / 255.0\n",
    "\n",
    "# Menambahkan dimensi saluran (channel) untuk LBP\n",
    "lbp_images = lbp_images.reshape(lbp_images.shape + (1,))\n",
    "\n",
    "# Mengonversi citra asli menjadi array dengan dimensi (tinggi x lebar x 1)\n",
    "original_images = np.array(pixels.values.tolist())[..., np.newaxis]\n",
    "\n",
    "# Menggabungkan hasil LBP dengan citra asli\n",
    "combined_images = np.concatenate((original_images, lbp_images), axis=3)\n",
    "\n",
    "# Membagi data menjadi data pelatihan, validasi, dan pengujian\n",
    "x_train, x_test, y_train, y_test = train_test_split(combined_images, labels, test_size=0.2, random_state=42)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Membangun model CNN\n",
    "model = tf.keras.Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 2)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(7, activation='softmax')  # Jumlah kelas pada FER2013 adalah 7\n",
    "])\n",
    "\n",
    "# # Membuat model VGG16 dengan citra grayscale\n",
    "# model = Sequential()\n",
    "\n",
    "# # Layer 1 (grayscale input)\n",
    "# model.add(Conv2D(64, (3, 3), input_shape=(48, 48, 1), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# # Layer 2\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# # Layer 3\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# # Layer 4\n",
    "# model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# # Layer 5\n",
    "# model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# # Fully connected layers\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(4096, activation='relu'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(7, activation='softmax'))  # 1000 classes for ImageNet\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Melatih model\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_valid, y_valid))\n",
    "\n",
    "# Evaluasi model pada data pengujian\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dda91c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "449/449 [==============================] - 3s 6ms/step - loss: 1.9099 - accuracy: 0.3176 - val_loss: 1.5918 - val_accuracy: 0.3806\n",
      "Epoch 2/10\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 1.3750 - accuracy: 0.4851 - val_loss: 1.5112 - val_accuracy: 0.4218\n",
      "Epoch 3/10\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 1.1267 - accuracy: 0.5845 - val_loss: 1.5820 - val_accuracy: 0.4193\n",
      "Epoch 4/10\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.9173 - accuracy: 0.6706 - val_loss: 1.7121 - val_accuracy: 0.4157\n",
      "Epoch 5/10\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.7324 - accuracy: 0.7395 - val_loss: 1.8882 - val_accuracy: 0.4101\n",
      "Epoch 6/10\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.5608 - accuracy: 0.8050 - val_loss: 2.2725 - val_accuracy: 0.4164\n",
      "Epoch 7/10\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.4035 - accuracy: 0.8686 - val_loss: 2.5824 - val_accuracy: 0.3989\n",
      "Epoch 8/10\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.2777 - accuracy: 0.9139 - val_loss: 3.0910 - val_accuracy: 0.4012\n",
      "Epoch 9/10\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.1777 - accuracy: 0.9505 - val_loss: 3.5510 - val_accuracy: 0.3926\n",
      "Epoch 10/10\n",
      "449/449 [==============================] - 2s 5ms/step - loss: 0.1327 - accuracy: 0.9653 - val_loss: 3.8722 - val_accuracy: 0.3969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b16350e1d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Function to calculate LBP for a single image\n",
    "def calculate_lbp_image(img):\n",
    "    lbp_image = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Read the FER2013 dataset\n",
    "data = pd.read_csv('../fer2013.csv')\n",
    "\n",
    "# Extract LBP features and store in a list\n",
    "lbp_features = []\n",
    "for i in range(len(data)):\n",
    "    pixels = np.array(data['pixels'].iloc[i].split(' '), dtype=int)\n",
    "    img = pixels.reshape(48, 48)\n",
    "    lbp_image = calculate_lbp_image(img)\n",
    "    lbp_features.append(lbp_image)\n",
    "\n",
    "# Convert the list to an array\n",
    "lbp_features = np.array(lbp_features)\n",
    "\n",
    "# Convert emotion labels to one-hot encoding\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(data['emotion'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=7)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(lbp_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a CNN model\n",
    "input_layer = Input(shape=(48, 48, 1))\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu')(input_layer)\n",
    "pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "flatten = Flatten()(pool1)\n",
    "dense1 = Dense(128, activation='relu')(flatten)\n",
    "output_layer = Dense(7, activation='softmax')(dense1)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape LBP data to fit the model input\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Train the model with LBP data\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8ae8125",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[36864,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m train_test_split(lbp_features, labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Buat model CNN\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m48\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMaxPooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpool_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# ... [Add other layers as required]\u001b[39;49;00m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Output layer for 7 emotions\u001b[39;49;00m\n\u001b[0;32m     50\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Compile model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\backend.py:2100\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype, nonce)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonce:\n\u001b[0;32m   2099\u001b[0m         seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mstateless_fold_in(seed, nonce)\n\u001b[1;32m-> 2100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateless_uniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m   2108\u001b[0m     shape\u001b[38;5;241m=\u001b[39mshape,\n\u001b[0;32m   2109\u001b[0m     minval\u001b[38;5;241m=\u001b[39mminval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2112\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_legacy_seed(),\n\u001b[0;32m   2113\u001b[0m )\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[36864,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Fungsi untuk menghitung LBP pada satu gambar\n",
    "def calculate_lbp_image(img):\n",
    "    lbp_image = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Baca dataset FER2013\n",
    "data = pd.read_csv('../fer2013.csv')\n",
    "\n",
    "# Ekstraksi fitur LBP dan simpan dalam list\n",
    "lbp_features = []\n",
    "for i in range(len(data)):\n",
    "    pixels = np.array(data['pixels'].iloc[i].split(' '), dtype=int)\n",
    "    img = pixels.reshape(48, 48)\n",
    "    lbp_image = calculate_lbp_image(img)\n",
    "    lbp_features.append(lbp_image)\n",
    "\n",
    "# Konversi list ke array\n",
    "lbp_features = np.array(lbp_features)\n",
    "\n",
    "# Konversi label emosi menjadi one-hot encoding\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(data['emotion'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=7)  # 7 classes for 7 emotions\n",
    "\n",
    "# Bagi data menjadi set pelatihan dan validasi\n",
    "X_train, X_val, y_train, y_val = train_test_split(lbp_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Buat model CNN\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), input_shape=(48, 48, 1), activation='relu', padding='same'),\n",
    "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "    # ... [Add other layers as required]\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')  # Output layer for 7 emotions\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape data LBP untuk memenuhi input model\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Latih model dengan data LBP\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f8a82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "449/449 [==============================] - ETA: 0s - loss: 1.8190 - accuracy: 0.2494"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 13). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.h5py\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: best_model.h5py\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449/449 [==============================] - 46s 99ms/step - loss: 1.8190 - accuracy: 0.2494 - val_loss: 1.8122 - val_accuracy: 0.2459\n",
      "Epoch 2/20\n",
      "449/449 [==============================] - 41s 92ms/step - loss: 1.8144 - accuracy: 0.2516 - val_loss: 1.8104 - val_accuracy: 0.2459\n",
      "Epoch 3/20\n",
      "449/449 [==============================] - 42s 93ms/step - loss: 1.8128 - accuracy: 0.2516 - val_loss: 1.8099 - val_accuracy: 0.2459\n",
      "Epoch 4/20\n",
      "449/449 [==============================] - 42s 93ms/step - loss: 1.8127 - accuracy: 0.2516 - val_loss: 1.8111 - val_accuracy: 0.2459\n",
      "Epoch 5/20\n",
      "449/449 [==============================] - 42s 94ms/step - loss: 1.8129 - accuracy: 0.2516 - val_loss: 1.8100 - val_accuracy: 0.2459\n",
      "Epoch 6/20\n",
      "449/449 [==============================] - 42s 94ms/step - loss: 1.8126 - accuracy: 0.2516 - val_loss: 1.8109 - val_accuracy: 0.2459\n",
      "Epoch 7/20\n",
      "449/449 [==============================] - 43s 95ms/step - loss: 1.8124 - accuracy: 0.2516 - val_loss: 1.8111 - val_accuracy: 0.2459\n",
      "Epoch 8/20\n",
      "449/449 [==============================] - 42s 94ms/step - loss: 1.8121 - accuracy: 0.2516 - val_loss: 1.8111 - val_accuracy: 0.2459\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Fungsi untuk menghitung LBP pada satu gambar\n",
    "def calculate_lbp_image(img):\n",
    "    lbp_image = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Baca dataset FER2013\n",
    "data = pd.read_csv('../fer2013.csv')\n",
    "\n",
    "# Ekstraksi fitur LBP dan simpan dalam list\n",
    "lbp_features = []\n",
    "for i in range(len(data)):\n",
    "    pixels = np.array(data['pixels'].iloc[i].split(' '), dtype=int)\n",
    "    img = pixels.reshape(48, 48)\n",
    "    lbp_image = calculate_lbp_image(img)\n",
    "    lbp_features.append(lbp_image)\n",
    "\n",
    "# Konversi list ke array\n",
    "lbp_features = np.array(lbp_features)\n",
    "\n",
    "# Konversi label emosi menjadi one-hot encoding\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(data['emotion'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=7)  # num_classes sesuai dengan jumlah kelas emosi\n",
    "\n",
    "# Bagi data menjadi set pelatihan dan validasi\n",
    "X_train, X_val, y_train, y_val = train_test_split(lbp_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Buat model CNN dengan arsitektur yang diberikan\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(48, 48, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))  # Output layer with 7 classes (emotions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callback untuk early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Callback untuk menyimpan model terbaik\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5py', save_best_only=True, save_weights_only=False, monitor='val_accuracy', mode='max')\n",
    "\n",
    "# Reshape data LBP untuk memenuhi input model\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Latih model dengan data LBP\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "# Selanjutnya, Anda dapat menggunakan model ini untuk inferensi atau melanjutkan pelatihan sesuai kebutuhan Anda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f7df46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "189/189 [==============================] - 18s 77ms/step - loss: 0.7159 - accuracy: 0.5961 - val_loss: 0.6813 - val_accuracy: 0.5816\n",
      "Epoch 2/10\n",
      "189/189 [==============================] - 13s 69ms/step - loss: 0.6738 - accuracy: 0.6004 - val_loss: 0.6816 - val_accuracy: 0.5816\n",
      "Epoch 3/10\n",
      "189/189 [==============================] - 13s 69ms/step - loss: 0.6737 - accuracy: 0.6004 - val_loss: 0.6800 - val_accuracy: 0.5816\n",
      "Epoch 4/10\n",
      "189/189 [==============================] - 13s 69ms/step - loss: 0.6742 - accuracy: 0.6004 - val_loss: 0.6801 - val_accuracy: 0.5816\n",
      "Epoch 5/10\n",
      "189/189 [==============================] - 13s 69ms/step - loss: 0.6741 - accuracy: 0.6004 - val_loss: 0.6798 - val_accuracy: 0.5816\n",
      "Epoch 6/10\n",
      "189/189 [==============================] - 13s 70ms/step - loss: 0.6737 - accuracy: 0.6004 - val_loss: 0.6809 - val_accuracy: 0.5816\n",
      "Epoch 7/10\n",
      "189/189 [==============================] - 13s 71ms/step - loss: 0.6733 - accuracy: 0.6004 - val_loss: 0.6843 - val_accuracy: 0.5816\n",
      "Epoch 8/10\n",
      "189/189 [==============================] - 13s 71ms/step - loss: 0.6733 - accuracy: 0.6004 - val_loss: 0.6802 - val_accuracy: 0.5816\n",
      "Epoch 9/10\n",
      "189/189 [==============================] - 13s 70ms/step - loss: 0.6738 - accuracy: 0.6004 - val_loss: 0.6807 - val_accuracy: 0.5816\n",
      "Epoch 10/10\n",
      "189/189 [==============================] - 13s 71ms/step - loss: 0.6732 - accuracy: 0.6004 - val_loss: 0.6810 - val_accuracy: 0.5816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b16343fa00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Fungsi untuk menghitung LBP pada satu gambar\n",
    "def calculate_lbp_image(img):\n",
    "    lbp_image = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Baca dataset FER2013\n",
    "data = pd.read_csv('../fer2013.csv')\n",
    "\n",
    "# Filter dataset untuk mengambil hanya data yang berhubungan dengan emosi senang (happy) dan sedih (sad)\n",
    "filtered_data = data[(data['emotion'] == 3) | (data['emotion'] == 4)]\n",
    "\n",
    "# Ekstraksi fitur LBP dan simpan dalam list\n",
    "lbp_features = []\n",
    "for i in range(len(filtered_data)):\n",
    "    pixels = np.array(filtered_data['pixels'].iloc[i].split(' '), dtype=int)\n",
    "    img = pixels.reshape(48, 48)\n",
    "    lbp_image = calculate_lbp_image(img)\n",
    "    lbp_features.append(lbp_image)\n",
    "\n",
    "# Konversi list ke array\n",
    "lbp_features = np.array(lbp_features)\n",
    "\n",
    "# Konversi label emosi menjadi one-hot encoding\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(filtered_data['emotion'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=2)\n",
    "\n",
    "# Bagi data menjadi set pelatihan dan validasi\n",
    "X_train, X_val, y_train, y_val = train_test_split(lbp_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Buat model Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(48, 48, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))  # Output layer with 2 classes (happy and sad)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape data LBP untuk memenuhi input model\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Latih model dengan data LBP\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff17a10",
   "metadata": {},
   "source": [
    "# Dipake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95ba18d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "449/449 [==============================] - 32s 59ms/step - loss: 2.6304 - accuracy: 0.2853 - val_loss: 1.6577 - val_accuracy: 0.3494\n",
      "Epoch 2/10\n",
      "449/449 [==============================] - 26s 59ms/step - loss: 1.6142 - accuracy: 0.3643 - val_loss: 1.6022 - val_accuracy: 0.3646\n",
      "Epoch 3/10\n",
      "449/449 [==============================] - 27s 59ms/step - loss: 1.5229 - accuracy: 0.4030 - val_loss: 1.5890 - val_accuracy: 0.3794\n",
      "Epoch 4/10\n",
      "449/449 [==============================] - 27s 59ms/step - loss: 1.3896 - accuracy: 0.4623 - val_loss: 1.5709 - val_accuracy: 0.3963\n",
      "Epoch 5/10\n",
      "449/449 [==============================] - 27s 60ms/step - loss: 1.1761 - accuracy: 0.5528 - val_loss: 1.5991 - val_accuracy: 0.3866\n",
      "Epoch 6/10\n",
      "449/449 [==============================] - 27s 60ms/step - loss: 0.8894 - accuracy: 0.6753 - val_loss: 1.7813 - val_accuracy: 0.3749\n",
      "Epoch 7/10\n",
      "449/449 [==============================] - 27s 60ms/step - loss: 0.5193 - accuracy: 0.8189 - val_loss: 2.0085 - val_accuracy: 0.3828\n",
      "Epoch 8/10\n",
      "449/449 [==============================] - 27s 60ms/step - loss: 0.2856 - accuracy: 0.9044 - val_loss: 2.1796 - val_accuracy: 0.3898\n",
      "Epoch 9/10\n",
      "449/449 [==============================] - 27s 60ms/step - loss: 0.2012 - accuracy: 0.9385 - val_loss: 2.6260 - val_accuracy: 0.3924\n",
      "Epoch 10/10\n",
      "449/449 [==============================] - 27s 60ms/step - loss: 0.1560 - accuracy: 0.9535 - val_loss: 2.7997 - val_accuracy: 0.3788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bd3be93b50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dipakek\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Fungsi untuk menghitung LBP pada satu gambar\n",
    "def calculate_lbp_image(img):\n",
    "    lbp_image = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Baca dataset FER2013\n",
    "data = pd.read_csv('../fer2013.csv')\n",
    "\n",
    "# Ekstraksi fitur LBP dan simpan dalam list\n",
    "lbp_features = []\n",
    "for i in range(len(data)):\n",
    "    pixels = np.array(data['pixels'].iloc[i].split(' '), dtype=int)\n",
    "    img = pixels.reshape(48, 48)\n",
    "    lbp_image = calculate_lbp_image(img)\n",
    "    lbp_features.append(lbp_image)\n",
    "\n",
    "# Konversi list ke array\n",
    "lbp_features = np.array(lbp_features)\n",
    "\n",
    "# Konversi label emosi menjadi one-hot encoding\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(data['emotion'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=7)  # Ubah menjadi 7 karena ada 7 emosi dalam dataset\n",
    "\n",
    "# Bagi data menjadi set pelatihan dan validasi\n",
    "X_train, X_val, y_train, y_val = train_test_split(lbp_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Buat model Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(48, 48, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))  # Output layer with 7 classes (emotions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Reshape data LBP untuk memenuhi input model\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Latih model dengan data LBP\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6513ba80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "449/449 [==============================] - 37s 76ms/step - loss: 1.8320 - accuracy: 0.2494 - val_loss: 1.8115 - val_accuracy: 0.2459\n",
      "Epoch 2/10\n",
      "449/449 [==============================] - 33s 73ms/step - loss: 1.8146 - accuracy: 0.2516 - val_loss: 1.8135 - val_accuracy: 0.2459\n",
      "Epoch 3/10\n",
      "449/449 [==============================] - 33s 73ms/step - loss: 1.8135 - accuracy: 0.2516 - val_loss: 1.8139 - val_accuracy: 0.2459\n",
      "Epoch 4/10\n",
      "449/449 [==============================] - 33s 74ms/step - loss: 1.8124 - accuracy: 0.2516 - val_loss: 1.8112 - val_accuracy: 0.2459\n",
      "Epoch 5/10\n",
      "449/449 [==============================] - 34s 75ms/step - loss: 1.8125 - accuracy: 0.2516 - val_loss: 1.8097 - val_accuracy: 0.2459\n",
      "Epoch 6/10\n",
      "449/449 [==============================] - 33s 74ms/step - loss: 1.8127 - accuracy: 0.2516 - val_loss: 1.8100 - val_accuracy: 0.2459\n",
      "Epoch 7/10\n",
      "449/449 [==============================] - 33s 74ms/step - loss: 1.8125 - accuracy: 0.2516 - val_loss: 1.8118 - val_accuracy: 0.2459\n",
      "Epoch 8/10\n",
      "449/449 [==============================] - 34s 76ms/step - loss: 1.8127 - accuracy: 0.2516 - val_loss: 1.8106 - val_accuracy: 0.2459\n",
      "Epoch 9/10\n",
      "449/449 [==============================] - 34s 75ms/step - loss: 1.8118 - accuracy: 0.2516 - val_loss: 1.8097 - val_accuracy: 0.2459\n",
      "Epoch 10/10\n",
      "449/449 [==============================] - 34s 75ms/step - loss: 1.8121 - accuracy: 0.2516 - val_loss: 1.8100 - val_accuracy: 0.2459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bd3adc1de0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Fungsi untuk menghitung LBP pada satu gambar\n",
    "def calculate_lbp_image(img):\n",
    "    lbp_image = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Baca dataset FER2013\n",
    "data = pd.read_csv('../fer2013.csv')\n",
    "\n",
    "# Ekstraksi fitur LBP dan simpan dalam list\n",
    "lbp_features = []\n",
    "for i in range(len(data)):\n",
    "    pixels = np.array(data['pixels'].iloc[i].split(' '), dtype=int)\n",
    "    img = pixels.reshape(48, 48)\n",
    "    lbp_image = calculate_lbp_image(img)\n",
    "    lbp_features.append(lbp_image)\n",
    "\n",
    "# Konversi list ke array\n",
    "lbp_features = np.array(lbp_features)\n",
    "\n",
    "# Konversi label emosi menjadi one-hot encoding\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(data['emotion'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=7)  # Ubah menjadi 7 karena ada 7 emosi dalam dataset\n",
    "\n",
    "# Bagi data menjadi set pelatihan dan validasi\n",
    "X_train, X_val, y_train, y_val = train_test_split(lbp_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Buat model Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(48, 48, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))  # Output layer with 7 classes (emotions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape data LBP untuk memenuhi input model\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Latih model dengan data LBP\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ef2e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "449/449 [==============================] - 27s 59ms/step - loss: 1.8223 - accuracy: 0.2457 - val_loss: 1.8117 - val_accuracy: 0.2459\n",
      "Epoch 2/10\n",
      "449/449 [==============================] - 26s 58ms/step - loss: 1.8136 - accuracy: 0.2516 - val_loss: 1.8131 - val_accuracy: 0.2459\n",
      "Epoch 3/10\n",
      "449/449 [==============================] - 26s 58ms/step - loss: 1.8135 - accuracy: 0.2516 - val_loss: 1.8101 - val_accuracy: 0.2459\n",
      "Epoch 4/10\n",
      "449/449 [==============================] - 26s 59ms/step - loss: 1.8125 - accuracy: 0.2516 - val_loss: 1.8106 - val_accuracy: 0.2459\n",
      "Epoch 5/10\n",
      "449/449 [==============================] - 26s 59ms/step - loss: 1.8122 - accuracy: 0.2516 - val_loss: 1.8106 - val_accuracy: 0.2459\n",
      "Epoch 6/10\n",
      "449/449 [==============================] - 27s 60ms/step - loss: 1.8127 - accuracy: 0.2516 - val_loss: 1.8108 - val_accuracy: 0.2459\n",
      "Epoch 7/10\n",
      "449/449 [==============================] - 27s 61ms/step - loss: 1.8125 - accuracy: 0.2516 - val_loss: 1.8108 - val_accuracy: 0.2459\n",
      "Epoch 8/10\n",
      "449/449 [==============================] - 28s 62ms/step - loss: 1.8120 - accuracy: 0.2516 - val_loss: 1.8098 - val_accuracy: 0.2459\n",
      "Epoch 9/10\n",
      "449/449 [==============================] - 26s 59ms/step - loss: 1.8126 - accuracy: 0.2516 - val_loss: 1.8101 - val_accuracy: 0.2459\n",
      "Epoch 10/10\n",
      "449/449 [==============================] - 27s 59ms/step - loss: 1.8123 - accuracy: 0.2516 - val_loss: 1.8103 - val_accuracy: 0.2459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bd530a70a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Fungsi untuk menghitung LBP pada satu gambar\n",
    "def calculate_lbp_image(img):\n",
    "    lbp_image = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Baca dataset FER2013\n",
    "data = pd.read_csv('../fer2013.csv')\n",
    "\n",
    "# Ekstraksi fitur LBP dan simpan dalam list\n",
    "lbp_features = []\n",
    "for i in range(len(data)):\n",
    "    pixels = np.array(data['pixels'].iloc[i].split(' '), dtype=int)\n",
    "    img = pixels.reshape(48, 48)\n",
    "    lbp_image = calculate_lbp_image(img)\n",
    "    lbp_features.append(lbp_image)\n",
    "\n",
    "# Konversi list ke array\n",
    "lbp_features = np.array(lbp_features)\n",
    "\n",
    "# Konversi label emosi menjadi one-hot encoding\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(data['emotion'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=7)  # Ubah menjadi 7 karena ada 7 emosi dalam dataset\n",
    "\n",
    "# Bagi data menjadi set pelatihan dan validasi\n",
    "X_train, X_val, y_train, y_val = train_test_split(lbp_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Buat model Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(48, 48, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))  # Output layer with 7 classes (emotions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape data LBP untuk memenuhi input model\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Latih model dengan data LBP\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803e011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "449/449 [==============================] - 61s 133ms/step - loss: 1.8211 - accuracy: 0.2484 - val_loss: 1.8105 - val_accuracy: 0.2459\n",
      "Epoch 2/20\n",
      "449/449 [==============================] - 62s 139ms/step - loss: 1.8135 - accuracy: 0.2516 - val_loss: 1.8119 - val_accuracy: 0.2459\n",
      "Epoch 3/20\n",
      "449/449 [==============================] - 62s 139ms/step - loss: 1.8134 - accuracy: 0.2516 - val_loss: 1.8106 - val_accuracy: 0.2459\n",
      "Epoch 4/20\n",
      "449/449 [==============================] - 63s 140ms/step - loss: 1.8125 - accuracy: 0.2516 - val_loss: 1.8105 - val_accuracy: 0.2459\n",
      "Epoch 5/20\n",
      "449/449 [==============================] - 63s 139ms/step - loss: 1.8125 - accuracy: 0.2516 - val_loss: 1.8102 - val_accuracy: 0.2459\n",
      "Epoch 6/20\n",
      "449/449 [==============================] - 62s 139ms/step - loss: 1.8124 - accuracy: 0.2516 - val_loss: 1.8096 - val_accuracy: 0.2459\n",
      "Epoch 7/20\n",
      "449/449 [==============================] - 65s 145ms/step - loss: 1.8121 - accuracy: 0.2516 - val_loss: 1.8111 - val_accuracy: 0.2459\n",
      "Epoch 8/20\n",
      "449/449 [==============================] - 62s 138ms/step - loss: 1.8123 - accuracy: 0.2516 - val_loss: 1.8096 - val_accuracy: 0.2459\n",
      "Epoch 9/20\n",
      "404/449 [=========================>....] - ETA: 5s - loss: 1.8127 - accuracy: 0.2518"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m X_val \u001b[38;5;241m=\u001b[39m X_val\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m48\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Latih model dengan data LBP\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\.conda\\envs\\tensorflowgpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Fungsi untuk menghitung LBP pada satu gambar\n",
    "def calculate_lbp_image(img):\n",
    "    lbp_image = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Baca dataset FER2013\n",
    "data = pd.read_csv('../fer2013.csv')\n",
    "\n",
    "# Ekstraksi fitur LBP dan simpan dalam list\n",
    "lbp_features = []\n",
    "for i in range(len(data)):\n",
    "    pixels = np.array(data['pixels'].iloc[i].split(' '), dtype=int)\n",
    "    img = pixels.reshape(48, 48)\n",
    "    lbp_image = calculate_lbp_image(img)\n",
    "    lbp_features.append(lbp_image)\n",
    "\n",
    "# Konversi list ke array\n",
    "lbp_features = np.array(lbp_features)\n",
    "\n",
    "# Konversi label emosi menjadi one-hot encoding\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(data['emotion'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=7)  # Ubah menjadi 7 karena ada 7 emosi dalam dataset\n",
    "\n",
    "# Bagi data menjadi set pelatihan dan validasi\n",
    "X_train, X_val, y_train, y_val = train_test_split(lbp_features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Buat model Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(48, 48, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))  # Mengurangi jumlah unit\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))  # Output layer with 7 classes (emotions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape data LBP untuk memenuhi input model\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Latih model dengan data LBP\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)  # Menambah jumlah epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f56175e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "337/337 [==============================] - 21s 59ms/step - loss: 1.8306 - accuracy: 0.2408 - val_loss: 1.8104 - val_accuracy: 0.2502\n",
      "Epoch 2/20\n",
      "337/337 [==============================] - 20s 60ms/step - loss: 1.8148 - accuracy: 0.2496 - val_loss: 1.8145 - val_accuracy: 0.2502\n",
      "Epoch 3/20\n",
      "337/337 [==============================] - 19s 57ms/step - loss: 1.8129 - accuracy: 0.2498 - val_loss: 1.8105 - val_accuracy: 0.2502\n",
      "Epoch 4/20\n",
      "337/337 [==============================] - 19s 56ms/step - loss: 1.8118 - accuracy: 0.2498 - val_loss: 1.8110 - val_accuracy: 0.2502\n",
      "Epoch 5/20\n",
      "337/337 [==============================] - 19s 56ms/step - loss: 1.8124 - accuracy: 0.2498 - val_loss: 1.8121 - val_accuracy: 0.2502\n",
      "Epoch 6/20\n",
      "337/337 [==============================] - 19s 56ms/step - loss: 1.8124 - accuracy: 0.2498 - val_loss: 1.8106 - val_accuracy: 0.2502\n",
      "Epoch 7/20\n",
      "337/337 [==============================] - 20s 59ms/step - loss: 1.8122 - accuracy: 0.2498 - val_loss: 1.8107 - val_accuracy: 0.2502\n",
      "Epoch 8/20\n",
      "337/337 [==============================] - 20s 59ms/step - loss: 1.8119 - accuracy: 0.2498 - val_loss: 1.8109 - val_accuracy: 0.2502\n",
      "Epoch 9/20\n",
      "337/337 [==============================] - 20s 59ms/step - loss: 1.8116 - accuracy: 0.2498 - val_loss: 1.8103 - val_accuracy: 0.2502\n",
      "Epoch 10/20\n",
      "337/337 [==============================] - 20s 59ms/step - loss: 1.8115 - accuracy: 0.2498 - val_loss: 1.8108 - val_accuracy: 0.2502\n",
      "Epoch 11/20\n",
      "337/337 [==============================] - 19s 56ms/step - loss: 1.8119 - accuracy: 0.2498 - val_loss: 1.8104 - val_accuracy: 0.2502\n",
      "Epoch 12/20\n",
      "337/337 [==============================] - 21s 61ms/step - loss: 1.8114 - accuracy: 0.2498 - val_loss: 1.8102 - val_accuracy: 0.2502\n",
      "Epoch 13/20\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 1.8117 - accuracy: 0.2498 - val_loss: 1.8104 - val_accuracy: 0.2502\n",
      "Epoch 14/20\n",
      "337/337 [==============================] - 21s 61ms/step - loss: 1.8122 - accuracy: 0.2498 - val_loss: 1.8103 - val_accuracy: 0.2502\n",
      "Epoch 15/20\n",
      "337/337 [==============================] - 20s 59ms/step - loss: 1.8112 - accuracy: 0.2498 - val_loss: 1.8112 - val_accuracy: 0.2502\n",
      "Epoch 16/20\n",
      "337/337 [==============================] - 20s 58ms/step - loss: 1.8117 - accuracy: 0.2498 - val_loss: 1.8102 - val_accuracy: 0.2502\n",
      "Epoch 17/20\n",
      "337/337 [==============================] - 21s 63ms/step - loss: 1.8122 - accuracy: 0.2498 - val_loss: 1.8102 - val_accuracy: 0.2502\n",
      "Epoch 18/20\n",
      "337/337 [==============================] - 21s 61ms/step - loss: 1.8112 - accuracy: 0.2498 - val_loss: 1.8102 - val_accuracy: 0.2502\n",
      "Epoch 19/20\n",
      "337/337 [==============================] - 20s 59ms/step - loss: 1.8114 - accuracy: 0.2498 - val_loss: 1.8108 - val_accuracy: 0.2502\n",
      "Epoch 20/20\n",
      "337/337 [==============================] - 19s 58ms/step - loss: 1.8114 - accuracy: 0.2498 - val_loss: 1.8100 - val_accuracy: 0.2502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bd540f4b20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Fungsi untuk menghitung LBP pada satu gambar\n",
    "def calculate_lbp_image(img):\n",
    "    lbp_image = local_binary_pattern(img, 8, 1, method='uniform')\n",
    "    return lbp_image\n",
    "\n",
    "# Baca dataset FER2013\n",
    "data = pd.read_csv('../fer2013.csv')\n",
    "\n",
    "# Ekstraksi fitur LBP dan simpan dalam list\n",
    "lbp_features = []\n",
    "for i in range(len(data)):\n",
    "    pixels = np.array(data['pixels'].iloc[i].split(' '), dtype=int)\n",
    "    img = pixels.reshape(48, 48)\n",
    "    lbp_image = calculate_lbp_image(img)\n",
    "    lbp_features.append(lbp_image)\n",
    "\n",
    "# Konversi list ke array\n",
    "lbp_features = np.array(lbp_features)\n",
    "\n",
    "# Konversi label emosi menjadi one-hot encoding\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(data['emotion'])\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=7)  # Ubah menjadi 7 karena ada 7 emosi dalam dataset\n",
    "\n",
    "# Bagi data menjadi set pelatihan, validasi, dan pengujian\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(lbp_features, labels, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Buat model Sequential\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(64, (3, 3), input_shape=(48, 48, 1), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))  # Mengurangi jumlah unit\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))  # Output layer with 7 classes (emotions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Reshape data LBP untuk memenuhi input model\n",
    "X_train = X_train.reshape(-1, 48, 48, 1)\n",
    "X_val = X_val.reshape(-1, 48, 48, 1)\n",
    "\n",
    "# Latih model dengan data LBP\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=64)  # Menambah jumlah epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3839a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
